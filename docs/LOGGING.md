# Buddha Korea Logging System

Production-grade logging infrastructure following industry best practices for privacy, analytics, and operational monitoring.

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI App    â”‚
â”‚  + loguru       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ writes
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  JSONL Log Files                â”‚
â”‚  - logs/qa_pairs.jsonl          â”‚
â”‚  - logs/usage.jsonl             â”‚
â”‚  - logs/app.log                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ rotated by
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Logrotate (daily)              â”‚
â”‚  - Compress old logs            â”‚
â”‚  - Trigger analytics processing â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ processes
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Analytics Processor            â”‚
â”‚  - Incremental JSONL parsing    â”‚
â”‚  - Byte-offset checkpoints      â”‚
â”‚  - Atomic Redis updates (Lua)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ stores in
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Redis (AOF persistence)        â”‚
â”‚  - Daily query counts           â”‚
â”‚  - Token usage by model/mode    â”‚
â”‚  - Cost tracking                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Components

### 1. PII Protection (`privacy.py`)

Lightweight regex-based PII masking for Korean language data. **Automatically integrated** into both `usage_tracker.py` and `qa_logger.py` - all logged queries and responses are masked before writing to disk.

**Patterns:**
| PII Type | Example Input | Masked Output |
|----------|---------------|---------------|
| Korean phone | `010-1234-5678` | `[KOREAN_PHONE_MASKED]` |
| Korean RRN (ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸) | `901225-1234567` | `[KOREAN_RRN_MASKED]` |
| Email | `user@example.com` | `[EMAIL_MASKED]` |
| IP Address | `192.168.1.100` | `[IP_ADDRESS_MASKED]` |

**Integration Status:** âœ… **ACTIVE**
- `usage_tracker.py` - Masks query and response_preview before logging
- `qa_logger.py` - Masks full query and response before logging

**Manual Usage:**
```python
from privacy import mask_pii, anonymize_ip

# Mask PII in text
masked_text = mask_pii("ì „í™”ë²ˆí˜¸ëŠ” 010-1234-5678ì…ë‹ˆë‹¤")
# Output: "ì „í™”ë²ˆí˜¸ëŠ” [KOREAN_PHONE_MASKED]ì…ë‹ˆë‹¤"

# Anonymize IP (for nginx logs)
anon_ip = anonymize_ip("192.168.1.100")  # Returns: "192.168.1.0"
```

**Test:**
```bash
python privacy.py
```

### 2. Analytics Processor (`analytics_processor.py`)

Incremental log processor with Redis analytics aggregation.

**Features:**
- **File locking** (fcntl) - Prevents concurrent runs
- **Byte-offset checkpoints** - Resumable processing after rotation
- **Atomic Redis updates** - Lua scripts prevent race conditions
- **Daily aggregation** - Queries, costs, tokens by day/model/mode

**Usage:**
```bash
# Manual run
python analytics_processor.py

# Automated (runs every 30 minutes via cron)
# See "Post-Deployment Setup" below
```

**Redis Data Structure:**
```
analytics:daily:2025-11-24
  â”œâ”€ queries: 1234
  â”œâ”€ cost: 5.67
  â”œâ”€ tokens: 123456
  â”œâ”€ input_tokens: 98765
  â”œâ”€ output_tokens: 24691
  â””â”€ cached_queries: 456

analytics:model:gemini-2.5-pro
  â”œâ”€ queries: 800
  â”œâ”€ cost: 4.50
  â””â”€ tokens: 100000

analytics:mode:detailed
  â”œâ”€ queries: 234
  â”œâ”€ cost: 2.10
  â””â”€ tokens: 50000

analytics:sutra:diamond_sutra
  â””â”€ queries: 45
```

**Get Analytics:**
```python
from analytics_processor import AnalyticsProcessor

processor = AnalyticsProcessor()
analytics = processor.get_analytics(days=7)

print(analytics["totals"])
# {'queries': 5000, 'cost': 25.50, 'tokens': 500000, ...}
```

### 3. Log Rotation (`logrotate.d/`)

**App Logs** (`buddhakorea-app`):
- Rotates: `logs/*.log`, `logs/*.jsonl`
- Frequency: Daily (or when > 100MB)
- Retention: 30 days
- Compression: Yes (delayed)
- Post-rotate: Runs analytics processor

**Nginx Logs** (`buddhakorea-nginx`):
- Rotates: `/var/log/nginx/access.log`, `/var/log/nginx/error.log`
- Frequency: Daily
- Retention: 14 days
- Compression: Yes (delayed)

**Installation:**
```bash
sudo cp logrotate.d/buddhakorea-app /etc/logrotate.d/
sudo cp logrotate.d/buddhakorea-nginx /etc/logrotate.d/
```

**Test:**
```bash
sudo logrotate -d /etc/logrotate.d/buddhakorea-app
```

### 4. Redis Persistence (`redis.conf`)

**Configuration Highlights:**
- **Persistence**: AOF (appendonly.aof) + RDB snapshots
- **Memory limit**: 512MB with LRU eviction
- **Security**: Password authentication required
- **Dangerous commands disabled**: FLUSHALL, FLUSHDB, CONFIG

**Backup Script** (`backup_redis.sh`):
- Runs: Daily at 2 AM (via cron)
- Method: BGSAVE + tar.gz compression
- Retention: 30 days
- Location: `/opt/buddha-korea/backups/redis/`

**Manual Backup:**
```bash
./backup_redis.sh
```

### 5. Deployment Script (`deploy.sh`)

Blue-green deployment with health checks and rollback.

**Usage:**
```bash
sudo -u appuser ./deploy.sh
```

**Phases:**
1. **Preflight checks** - Verify disk space, files, permissions
2. **Backup** - Snapshot current deployment state
3. **Build** - Build new Docker images with --no-cache
4. **Deploy** - Blue-green swap with health checks
5. **Verify** - Test all services and endpoints
6. **Post-deployment** - Setup cron jobs, cleanup

**Rollback:**
Automatic rollback on failure. Manual rollback:
```bash
cd /opt/buddha-korea
docker-compose -f docker-compose.production.yml down
docker-compose -f docker-compose.production.yml up -d
```

## Verification Checklist

### âœ… 1. Container Log Paths Match Host Volume
```bash
# Check docker-compose.production.yml
grep -A1 "volumes:" docker-compose.production.yml | grep logs

# Expected output:
# - ./logs:/app/logs
```

**Status:** âœ… Verified - Container `/app/logs` maps to host `./logs`

### âœ… 2. PII Regex Tests Pass
```bash
python privacy.py
```

**Expected output:**
```
âœ… All PII masking tests passed (Korean language support verified)
```

**Status:** âœ… All tests pass with lookbehind/lookahead assertions for Unicode

### âœ… 2b. PII Masking Integrated into Loggers
```bash
# Test that loggers mask PII automatically
python -c "
from usage_tracker import log_token_usage
from qa_logger import log_qa_pair

# Log with PII - should be masked
log_token_usage(query='Call 010-1234-5678', response='OK', input_tokens=10, output_tokens=5, model='test')
log_qa_pair(query='Email me at test@example.com', response='Response with 010-9999-8888')

# Verify last entries are masked
import json
with open('logs/usage.jsonl') as f:
    last = json.loads(f.readlines()[-1])
    assert '[KOREAN_PHONE_MASKED]' in last['query'], 'PII not masked in usage_tracker'
print('âœ… PII masking integrated into loggers')
"
```

**Status:** âœ… `usage_tracker.py` and `qa_logger.py` automatically mask PII before logging

### âœ… 3. Lua Script for Atomic Redis Merges
```bash
# Test with sample data
python -c "
from analytics_processor import AnalyticsProcessor
import redis

# Create test Redis client
r = redis.Redis(host='localhost', port=6379, decode_responses=True)

# Create processor
processor = AnalyticsProcessor(redis_client=r)

# Check Lua script registration
print('Lua script registered:', processor.update_analytics_script.sha)
"
```

**Status:** âœ… Lua script in `analytics_processor.py` lines 36-76

### â³ 4. Dry-run Deployment Script
```bash
# Test on local machine first
cd /Users/vairocana/Desktop/buddhakorea/buddha-korea-notebook-exp/opennotebook

# Check preflight validations
bash -n deploy.sh  # Syntax check

# Inspect phases
grep -E "^(preflight_checks|backup_deployment|build_images|blue_green_deploy|post_deployment)" deploy.sh
```

**Status:** â³ Created, needs testing in staging environment

## Post-Deployment Setup

After running `deploy.sh`, verify the following cron jobs are configured:

### 1. Redis Backup (Daily at 2 AM)
```bash
crontab -l | grep backup_redis
# Expected: 0 2 * * * /opt/buddha-korea/backup_redis.sh
```

### 2. Analytics Processor (Every 30 minutes)
```bash
crontab -l | grep analytics_processor
# Expected: */30 * * * * cd /opt/buddha-korea && /usr/bin/python3 analytics_processor.py >> /opt/buddha-korea/logs/analytics_processor.log 2>&1
```

### 3. Logrotate (Daily via system cron)
```bash
ls -l /etc/logrotate.d/buddhakorea-*
# Expected:
# -rw-r--r-- 1 root root ... /etc/logrotate.d/buddhakorea-app
# -rw-r--r-- 1 root root ... /etc/logrotate.d/buddhakorea-nginx
```

## Monitoring Commands

### Check Log Files
```bash
# Q&A logs
tail -f logs/qa_pairs.jsonl | jq '.'

# Usage logs
tail -f logs/usage.jsonl | jq '.'

# Application logs
tail -f logs/app.log
```

### Check Redis Analytics
```bash
# Via Docker
docker exec buddhakorea-redis redis-cli -a ${REDIS_PASSWORD} INFO keyspace

# Get analytics via Python
docker exec buddhakorea-fastapi python3 -c "
from analytics_processor import AnalyticsProcessor
import json
print(json.dumps(AnalyticsProcessor().get_analytics(7), indent=2, ensure_ascii=False))
"
```

### Check Container Health
```bash
docker ps | grep buddhakorea
docker stats --no-stream buddhakorea-fastapi buddhakorea-redis buddhakorea-nginx
```

### Test Analytics Processing
```bash
# Create test log entry
echo '{"timestamp":"2025-11-24T12:00:00","model":"gemini-2.5-pro","cost_usd":0.01,"tokens":{"input":100,"output":50},"from_cache":false}' >> logs/usage.jsonl

# Run processor
python analytics_processor.py

# Verify in Redis
docker exec buddhakorea-redis redis-cli -a ${REDIS_PASSWORD} HGETALL analytics:daily:2025-11-24
```

## Security Considerations

1. **PII Masking**: All logs are masked before writing to disk
2. **IP Anonymization**: Last octet zeroed in nginx logs
3. **Redis Authentication**: Password required (set via REDIS_PASSWORD env var)
4. **Log Permissions**: 0640 (owner read/write, group read only)
5. **Dangerous Redis Commands Disabled**: FLUSHALL, FLUSHDB, CONFIG

## Troubleshooting

### Analytics not updating
```bash
# Check if processor is running
ps aux | grep analytics_processor

# Check for lock file (indicates another instance running)
ls -l logs/.analytics_processor.lock

# Check checkpoint file
cat logs/.analytics_checkpoint.json

# Run manually with verbose output
python analytics_processor.py
```

### Redis connection errors
```bash
# Test Redis connection
docker exec buddhakorea-redis redis-cli -a ${REDIS_PASSWORD} PING

# Check Redis logs
docker logs buddhakorea-redis

# Verify environment variables
docker exec buddhakorea-fastapi env | grep REDIS
```

### Log rotation not working
```bash
# Test logrotate config
sudo logrotate -d /etc/logrotate.d/buddhakorea-app

# Force rotation
sudo logrotate -f /etc/logrotate.d/buddhakorea-app

# Check system logrotate logs
sudo cat /var/log/syslog | grep logrotate
```

## Performance Impact

- **PII Masking**: < 5ms per query (acceptable overhead)
- **Analytics Processing**: Incremental, only processes new lines
- **Log Rotation**: Non-blocking (copytruncate mode)
- **Redis Queries**: < 1ms via Lua atomic operations

## Disk Usage Estimates

- **Daily logs**: ~50-200MB (compressed: ~10-40MB)
- **30 days retention**: ~300MB-1.2GB compressed
- **Redis AOF**: ~10-50MB (depends on query volume)
- **Redis backups**: ~5-25MB compressed per day

## Support

For issues or questions:
1. Check `logs/analytics_processor.log`
2. Check `logs/app.log`
3. Review Redis INFO: `docker exec buddhakorea-redis redis-cli -a ${REDIS_PASSWORD} INFO`
# Buddha Korea RAG - ì‚¬ìš©ëŸ‰ ì¶”ì  ê°€ì´ë“œ ğŸ“Š

**í† í° ì‚¬ìš©ëŸ‰ê³¼ API ë¹„ìš©ì„ ì¶”ì í•˜ëŠ” ì‹œìŠ¤í…œ**

---

## âœ¨ ê¸°ëŠ¥

âœ… **ê°œë³„ ì¿¼ë¦¬ í† í° ìˆ˜ ì¶”ì ** - ì…ë ¥/ì¶œë ¥ í† í° ê°œë³„ ê¸°ë¡
âœ… **ì‹¤ì‹œê°„ ë¹„ìš© ê³„ì‚°** - Gemini, Claude, GPT ëª¨ë¸ ëª¨ë‘ ì§€ì›
âœ… **ëª¨ë“œë³„ í†µê³„** - ì¼ë°˜/ìì„¸íˆ/ìºì‹œ ëª¨ë“œ êµ¬ë¶„
âœ… **ì¼ë³„/ëª¨ë¸ë³„ ë¶„ì„** - ì‹œê°„ëŒ€ë³„ ì‚¬ìš© íŒ¨í„´ ë¶„ì„
âœ… **CSV/JSON ë‚´ë³´ë‚´ê¸°** - ë°ì´í„° ë¶„ì„ ë° ë³´ê³ ì„œ ì‘ì„±

---

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
opennotebook/
â”œâ”€â”€ usage_tracker.py          # í•µì‹¬ ì¶”ì  ëª¨ë“ˆ
â”œâ”€â”€ main.py                    # í†µí•©ëœ FastAPI (ìë™ ë¡œê¹…)
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ usage.jsonl            # ì‚¬ìš©ëŸ‰ ë¡œê·¸ (JSON Lines í˜•ì‹)
â””â”€â”€ check_usage.sh             # CLI í†µê³„ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
```

---

## ğŸš€ ì‚¬ìš©ë²•

### 1. ìë™ ì¶”ì  (ê¸°ë³¸)

**ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤!**

`/api/chat` ì—”ë“œí¬ì¸íŠ¸ë¡œ ì¿¼ë¦¬ë¥¼ ë³´ë‚´ë©´ ìë™ìœ¼ë¡œ ë¡œê¹…ë©ë‹ˆë‹¤:

```bash
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "query": "ë¬´ìƒì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    "max_sources": 5
  }'
```

**ìë™ìœ¼ë¡œ ê¸°ë¡ë¨**:
- ì…ë ¥ í† í° ìˆ˜
- ì¶œë ¥ í† í° ìˆ˜
- ì˜ˆìƒ ë¹„ìš© (USD)
- ì¿¼ë¦¬ ëª¨ë“œ (normal/detailed/cached)
- ëª¨ë¸ëª…
- ì‘ë‹µ ì‹œê°„

---

### 2. CLIë¡œ í†µê³„ í™•ì¸

#### ë¹ ë¥¸ í™•ì¸:
```bash
./check_usage.sh
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```
ğŸ” Buddha Korea RAG - ì‚¬ìš©ëŸ‰ í†µê³„
==================================

ì´ ì¿¼ë¦¬ ìˆ˜: 15

ìµœê·¼ 5ê°œ ì¿¼ë¦¬:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Query: ë¬´ìƒì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”...
Mode: normal | Model: gemini-2.5-pro
Tokens: 8543in + 2011out | Cost: $0.030786

Query: ì‚¬ì„±ì œë¥¼ ìì„¸íˆ ì•Œë ¤ì¤˜...
Mode: detailed | Model: gemini-2.5-pro
Tokens: 16234in + 7892out | Cost: $0.099213

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ì´ ë¹„ìš©: $0.435678 USD

ëª¨ë“œë³„ í†µê³„:
  normal: 10íšŒ | $0.235678 | í‰ê· : $0.023568/query
  detailed: 5íšŒ | $0.200000 | í‰ê· : $0.040000/query
```

---

### 3. APIë¡œ ìƒì„¸ í†µê³„ ì¡°íšŒ

#### ê¸°ë³¸ í†µê³„ (ìµœê·¼ 7ì¼):
```bash
curl http://localhost:8000/api/usage-stats
```

**ì‘ë‹µ**:
```json
{
  "period_days": 7,
  "total_queries": 15,
  "cached_queries": 2,
  "api_queries": 13,
  "total_cost_usd": 0.4357,
  "tokens": {
    "input": 125430,
    "output": 38920,
    "total": 164350
  },
  "by_mode": {
    "normal": {
      "queries": 10,
      "cost_usd": 0.2357,
      "tokens": 98543,
      "avg_cost_per_query": 0.023570
    },
    "detailed": {
      "queries": 5,
      "cost_usd": 0.2000,
      "tokens": 65807,
      "avg_cost_per_query": 0.040000
    }
  },
  "by_day": {
    "2025-01-17": {
      "queries": 10,
      "cost_usd": 0.3000,
      "tokens": 120000
    },
    "2025-01-18": {
      "queries": 5,
      "cost_usd": 0.1357,
      "tokens": 44350
    }
  }
}
```

#### ìµœê·¼ 30ì¼ í†µê³„:
```bash
curl "http://localhost:8000/api/usage-stats?days=30"
```

#### CSV ë‹¤ìš´ë¡œë“œ:
```bash
curl "http://localhost:8000/api/usage-stats?format=csv" -o usage_stats.csv
```

---

### 4. ìµœê·¼ ì¿¼ë¦¬ ì¡°íšŒ

```bash
curl "http://localhost:8000/api/recent-queries?limit=10"
```

**ì‘ë‹µ**:
```json
{
  "count": 10,
  "queries": [
    {
      "timestamp": "2025-01-17T10:23:45",
      "query": "ë¬´ìƒì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
      "mode": "normal",
      "model": "gemini-2.5-pro",
      "tokens": {
        "input": 8543,
        "output": 2011,
        "total": 10554
      },
      "cost_usd": 0.030786,
      "from_cache": false,
      "latency_ms": 3245
    }
  ]
}
```

---

## ğŸ’° ë¹„ìš© ê³„ì‚° ë°©ì‹

### ì§€ì› ëª¨ë¸ ê°€ê²©í‘œ (per 1M tokens):

| ëª¨ë¸ | ì…ë ¥ | ì¶œë ¥ |
|------|------|------|
| **Gemini 2.5 Pro** | $1.25 | $10.00 |
| **Gemini 2.0 Flash** | $0 (Free) | $0.82 |
| **Gemini 1.5 Pro** | $1.25 | $5.00 |
| **Claude 3.5 Sonnet** | $3.00 | $15.00 |
| **GPT-4o** | $2.50 | $10.00 |

### ê³„ì‚° ê³µì‹:
```
ë¹„ìš© = (ì…ë ¥_í† í° / 1,000,000 Ã— ì…ë ¥_ê°€ê²©) + (ì¶œë ¥_í† í° / 1,000,000 Ã— ì¶œë ¥_ê°€ê²©)
```

### ì˜ˆì‹œ:
```
Gemini 2.5 Proë¡œ ì¼ë°˜ ì¿¼ë¦¬:
- ì…ë ¥: 8,500 í† í° â†’ $0.0106
- ì¶œë ¥: 2,048 í† í° â†’ $0.0205
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ì´ ë¹„ìš©: $0.0311
```

---

## ğŸ“Š ë¡œê·¸ íŒŒì¼ í˜•ì‹

**`logs/usage.jsonl`** (JSON Lines í˜•ì‹):

```jsonl
{"timestamp": "2025-01-17T10:23:45", "query": "ë¬´ìƒì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”", "response_preview": "ë¬´ìƒ(ç„¡å¸¸, anicca)ì€ ë¶ˆêµì˜ í•µì‹¬ ê°€ë¥´ì¹¨...", "mode": "normal", "model": "gemini-2.5-pro", "tokens": {"input": 8543, "output": 2011, "total": 10554}, "cost_usd": 0.030786, "from_cache": false, "session_id": "abc123", "latency_ms": 3245}
```

### í•„ë“œ ì„¤ëª…:
- `timestamp`: ISO 8601 í˜•ì‹ ì‹œê°„
- `query`: ì‚¬ìš©ì ì§ˆë¬¸ (ì²« 100ì)
- `response_preview`: LLM ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸° (ì²« 100ì)
- `mode`: `normal` | `detailed` | `cached`
- `model`: ì‚¬ìš©ëœ LLM ëª¨ë¸ëª…
- `tokens.input`: ì…ë ¥ í† í° ìˆ˜
- `tokens.output`: ì¶œë ¥ í† í° ìˆ˜
- `cost_usd`: ë¹„ìš© (USD, ì†Œìˆ˜ì  6ìë¦¬)
- `from_cache`: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
- `session_id`: ì„¸ì…˜ ID (optional)
- `latency_ms`: ì‘ë‹µ ì‹œê°„ (ë°€ë¦¬ì´ˆ)

---

## ğŸ”§ ê³ ê¸‰ ì‚¬ìš©ë²•

### 1. Pythonìœ¼ë¡œ ì§ì ‘ ë¶„ì„

```python
import json

# ì „ì²´ ë¡œê·¸ ì½ê¸°
with open('logs/usage.jsonl', 'r') as f:
    logs = [json.loads(line) for line in f]

# ì´ ë¹„ìš© ê³„ì‚°
total_cost = sum(log['cost_usd'] for log in logs)
print(f"ì´ ë¹„ìš©: ${total_cost:.4f}")

# ëª¨ë“œë³„ í‰ê·  ë¹„ìš©
from collections import defaultdict

mode_stats = defaultdict(lambda: {'count': 0, 'cost': 0.0})

for log in logs:
    mode = log['mode']
    mode_stats[mode]['count'] += 1
    mode_stats[mode]['cost'] += log['cost_usd']

for mode, stats in mode_stats.items():
    avg = stats['cost'] / stats['count']
    print(f"{mode}: {stats['count']}íšŒ, í‰ê·  ${avg:.6f}/query")
```

### 2. jqë¡œ ë¡œê·¸ ë¶„ì„

```bash
# ìµœê·¼ 10ê°œ ì¿¼ë¦¬ì˜ ë¹„ìš©
cat logs/usage.jsonl | tail -10 | jq '{query, cost: .cost_usd}'

# ì´ ë¹„ìš© ê³„ì‚°
cat logs/usage.jsonl | jq -s 'map(.cost_usd) | add'

# ëª¨ë“œë³„ í‰ê·  ë¹„ìš©
cat logs/usage.jsonl | jq -s '
  group_by(.mode) |
  map({
    mode: .[0].mode,
    avg_cost: (map(.cost_usd) | add / length)
  })
'

# ì¼ë³„ ë¹„ìš© í•©ê³„
cat logs/usage.jsonl | jq -s '
  group_by(.timestamp[:10]) |
  map({
    date: .[0].timestamp[:10],
    total_cost: (map(.cost_usd) | add)
  })
'
```

### 3. CSVë¡œ ë³€í™˜ í›„ Excel ë¶„ì„

```bash
# APIë¡œ CSV ë‹¤ìš´ë¡œë“œ
curl "http://localhost:8000/api/usage-stats?format=csv" -o usage_30days.csv

# Excelì´ë‚˜ Google Sheetsì—ì„œ ì—´ê¸°
open usage_30days.csv
```

---

## ğŸ“ˆ ëŒ€ì‹œë³´ë“œ í†µí•© (ì„ íƒì‚¬í•­)

### Grafana ì—°ë™ (ê¶Œì¥)

1. **Loki ì„¤ì¹˜** (ë¡œê·¸ ìˆ˜ì§‘)
```bash
docker run -d --name=loki -p 3100:3100 grafana/loki:latest
```

2. **Promtail ì„¤ì •** (logs/usage.jsonl ì „ì†¡)
```yaml
# promtail-config.yaml
server:
  http_listen_port: 9080

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://localhost:3100/loki/api/v1/push

scrape_configs:
  - job_name: usage
    static_configs:
      - targets:
          - localhost
        labels:
          job: buddha-korea-usage
          __path__: /path/to/logs/usage.jsonl
```

3. **Grafana ëŒ€ì‹œë³´ë“œ** ìƒì„±

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### 1. **í† í° ì¶”ì •ì˜ ì •í™•ë„**

í˜„ì¬ ì‹œìŠ¤í…œì€ ë‹¤ìŒ ë°©ë²•ìœ¼ë¡œ í† í°ì„ ì¶”ì í•©ë‹ˆë‹¤:

- **LangChain ì‘ë‹µ ë©”íƒ€ë°ì´í„°** (ê°€ì¥ ì •í™•)
  - LLMì´ ë°˜í™˜í•˜ëŠ” token_usage ì •ë³´ ì‚¬ìš©
  - Gemini APIëŠ” ì´ ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ

- **í´ë°±: í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ì •** (Â±10-20% ì˜¤ì°¨)
  - ë¬¸ì ìˆ˜ Ã· 2.5 = í† í° ìˆ˜ (ë‹¤êµ­ì–´ í‰ê· )
  - ì…ë ¥ ì»¨í…ìŠ¤íŠ¸: 10 chunks Ã— 800 tokens ê°€ì •

### 2. **ë¹„ìš©ì€ ì˜ˆìƒì¹˜ì…ë‹ˆë‹¤**

- ì‹¤ì œ ì²­êµ¬ëŠ” GCP/Anthropic/OpenAI ê³„ì •ì—ì„œ í™•ì¸í•˜ì„¸ìš”
- ì´ ì‹œìŠ¤í…œì€ **ì¶”ì  ë° ëª¨ë‹ˆí„°ë§ ëª©ì **ì…ë‹ˆë‹¤

### 3. **ë¡œê·¸ íŒŒì¼ ê´€ë¦¬**

```bash
# ë¡œê·¸ íŒŒì¼ í¬ê¸° í™•ì¸
du -h logs/usage.jsonl

# ì˜¤ë˜ëœ ë¡œê·¸ ì •ë¦¬ (30ì¼ ì´ìƒ)
python3 << EOF
import json
from datetime import datetime, timedelta

cutoff = (datetime.now() - timedelta(days=30)).isoformat()

with open('logs/usage.jsonl', 'r') as f:
    lines = [
        line for line in f
        if json.loads(line)['timestamp'] >= cutoff
    ]

with open('logs/usage.jsonl', 'w') as f:
    f.writelines(lines)
EOF
```

---

## ğŸ¯ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

### Scenario 1: ì›”ê°„ ë¹„ìš© ë¦¬í¬íŠ¸
```bash
# ì´ë²ˆ ë‹¬ ì´ ë¹„ìš© í™•ì¸
curl "http://localhost:8000/api/usage-stats?days=30" | jq '{
  total_cost: .total_cost_usd,
  queries: .total_queries,
  avg_per_query: (.total_cost_usd / .total_queries)
}'
```

### Scenario 2: ëª¨ë¸ ë¹„êµ
```bash
# ëª¨ë¸ë³„ ë¹„ìš© íš¨ìœ¨ì„± ë¹„êµ
curl "http://localhost:8000/api/usage-stats?days=7" | jq '.by_model'
```

### Scenario 3: ìºì‹œ íš¨ìœ¨ì„±
```bash
# ìºì‹œë¡œ ì ˆì•½í•œ ë¹„ìš© ê³„ì‚°
curl "http://localhost:8000/api/usage-stats?days=7" | jq '{
  cached: .cached_queries,
  api: .api_queries,
  cache_rate: (.cached_queries / .total_queries * 100)
}'
```

---

## ğŸ“ ë¬¸ì œ í•´ê²°

### Q: ë¡œê·¸ê°€ ê¸°ë¡ë˜ì§€ ì•Šì•„ìš”
**A**:
1. `logs/` ë””ë ‰í† ë¦¬ ê¶Œí•œ í™•ì¸: `chmod 755 logs/`
2. `usage.jsonl` íŒŒì¼ í™•ì¸: `ls -lh logs/usage.jsonl`
3. FastAPI ë¡œê·¸ í™•ì¸: `docker compose logs fastapi`

### Q: í† í° ìˆ˜ê°€ 0ìœ¼ë¡œ ë‚˜ì™€ìš”
**A**:
- Gemini APIëŠ” í† í° ì •ë³´ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- í´ë°± ì¶”ì • ë¡œì§ì´ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
- `logs/app.log`ì—ì„œ "Token estimation" ë©”ì‹œì§€ í™•ì¸

### Q: ë¹„ìš©ì´ ë„ˆë¬´ ë†’ì•„ìš”!
**A**:
1. **ëª¨ë¸ ë³€ê²½**: Gemini 2.0 Flashë¡œ ì „í™˜ (92% ì ˆê°)
   ```bash
   # .env íŒŒì¼ì—ì„œ
   LLM_MODEL=gemini-2.0-flash-exp
   ```
2. **ìºì‹œ í™œìš©**: ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ì€ `/api/cache`ë¡œ ìºì‹±
3. **detailed ëª¨ë“œ ì œí•œ**: í•„ìš”í•  ë•Œë§Œ ì‚¬ìš©

---

**êµ¬í˜„ ì™„ë£Œ! ğŸ‰**

ì´ì œ ëª¨ë“  ì¿¼ë¦¬ì˜ í† í° ìˆ˜ì™€ ë¹„ìš©ì„ ì •í™•íˆ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
